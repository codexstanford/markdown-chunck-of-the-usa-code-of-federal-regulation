
# Title 14 - Aeronautics and Space
## Chapter I - Federal Aviation Administration, Department of Transportation
### Subchapter D - Airmen
#### PART 60 - FLIGHT SIMULATION TRAINING DEVICE INITIAL AND CONTINUING QUALIFICATION AND USE
##### Appendix A to Part 60 - Qualification Performance Standards for Airplane Full Flight Simulators
###### Guidance for Design and Integration of Aircraft Avionics Equipment in Simulators

(30) Aeronautical Radio, Inc. (ARINC) document 610,(as amended).

No additional regulatory or informational material applies to § 60.1, Applicability, or to § 60.2, Applicability of sponsor rules to persons who are not sponsors and who are engaged in certain unauthorized activities.

See Appendix F of this part for a list of definitions and abbreviations from part 1 and part 60, including the appropriate appendices of part 60.

No additional regulatory or informational material applies to § 60.4, Qualification Performance Standards.

See Appendix E of this part for additional regulatory and informational material regarding Quality Management Systems.

a. The intent of the language in § 60.7(b) is to have a specific FFS, identified by the sponsor, used at least once in an FAA-approved flight training program for the airplane simulated during the 12-month period described. The identification of the specific FFS may change from one 12-month period to the next 12-month period as long as the sponsor sponsors and uses at least one FFS at least once during the prescribed period. No minimum number of hours or minimum FFS periods are required.

b. The following examples describe acceptable operational practices:

(1) Example One.

(a) A sponsor is sponsoring a single, specific FFS for its own use, in its own facility or elsewhere-this single FFS forms the basis for the sponsorship. The sponsor uses that FFS at least once in each 12-month period in the sponsor's FAA-approved flight training program for the airplane simulated. This 12-month period is established according to the following schedule:

(i) If the FFS was qualified prior to May 30, 2008, the 12-month period begins on the date of the first continuing qualification evaluation conducted in accordance with § 60.19 after May 30, 2008, and continues for each subsequent 12-month period;

(ii) A device qualified on or after May 30, 2008, will be required to undergo an initial or upgrade evaluation in accordance with § 60.15. Once the initial or upgrade evaluation is complete, the first continuing qualification evaluation will be conducted within 6 months. The 12-month continuing qualification evaluation cycle begins on that date and continues for each subsequent 12-month period.

(b) There is no minimum number of hours of FFS use required.

(c) The identification of the specific FFS may change from one 12-month period to the next 12-month period as long as the sponsor sponsors and uses at least one FFS at least once during the prescribed period.

(2) Example Two.

(a) A sponsor sponsors an additional number of FFSs, in its facility or elsewhere. Each additionally sponsored FFS must be-

(i) Used by the sponsor in the sponsor's FAA-approved flight training program for the airplane simulated (as described in § 60.7(d)(1));

OR

(ii) Used by another FAA certificate holder in that other certificate holder's FAA-approved flight training program for the airplane simulated (as described in § 60.7(d)(1)). This 12-month period is established in the same manner as in example one;

OR

(iii) Provided a statement each year from a qualified pilot (after having flown the airplane, not the subject FFS or another FFS, during the preceding 12-month period), stating that the subject FFS's performance and handling qualities represent the airplane (as described in § 60.7(d)(2)). This statement is provided at least once in each 12-month period established in the same manner as in example one.

(b) No minimum number of hours of FFS use is required.

(3) Example Three.

(a) A sponsor in New York (in this example, a Part 142 certificate holder) establishes "satellite" training centers in Chicago and Moscow.

(b) The satellite function means that the Chicago and Moscow centers must operate under the New York center's certificate (in accordance with all of the New York center's practices, procedures, and policies; e.g., instructor and/or technician training/checking requirements, record keeping, QMS program).

(c) All of the FFSs in the Chicago and Moscow centers could be dry-leased (i.e., the certificate holder does not have and use FAA-approved flight training programs for the FFSs in the Chicago and Moscow centers) because-

(i) Each FFS in the Chicago center and each FFS in the Moscow center is used at least once each 12-month period by another FAA certificate holder in that other certificate holder's FAA-approved flight training program for the airplane (as described in § 60.7(d)(1));

OR

(ii) A statement is obtained from a qualified pilot (having flown the airplane, not the subject FFS or another FFS, during the preceding 12-month period) stating that the performance and handling qualities of each FFS in the Chicago and Moscow centers represents the airplane (as described in § 60.7(d)(2)).

The phrase "as soon as practicable" in § 60.9(a) means without unnecessarily disrupting or delaying beyond a reasonable time the training, evaluation, or experience being conducted in the FFS.

No additional regulatory or informational material applies to § 60.11, Simulator Use.

a. Flight test data used to validate FFS performance and handling qualities must have been gathered in accordance with a flight test program containing the following:

(1) A flight test plan consisting of:

(a) The maneuvers and procedures required for aircraft certification and simulation programming and validation.

(b) For each maneuver or procedure-

(i) The procedures and control input the flight test pilot and/or engineer used.

(ii) The atmospheric and environmental conditions.

(iii) The initial flight conditions.

(iv) The airplane configuration, including weight and center of gravity.

(v) The data to be gathered.

(vi) All other information necessary to recreate the flight test conditions in the FFS.

(2) Appropriately qualified flight test personnel.

(3) An understanding of the accuracy of the data to be gathered using appropriate alternative data sources, procedures, and instrumentation that is traceable to a recognized standard as described in Attachment 2, Table A2E of this appendix.

(4) Appropriate and sufficient data acquisition equipment or system(s), including appropriate data reduction and analysis methods and techniques, as would be acceptable to the FAA's Aircraft Certification Service.

b. The data, regardless of source, must be presented as follows:

(1) In a format that supports the FFS validation process.

(2) In a manner that is clearly readable and annotated correctly and completely.

(3) With resolution sufficient to determine compliance with the tolerances set forth in Attachment 2, Table A2A of this appendix.

(4) With any necessary instructions or other details provided, such as yaw damper or throttle position.

(5) Without alteration, adjustments, or bias. Data may be corrected to address known data calibration errors provided that an explanation of the methods used to correct the errors appears in the QTG. The corrected data may be re-scaled, digitized, or otherwise manipulated to fit the desired presentation.

c. After completion of any additional flight test, a flight test report must be submitted in support of the validation data. The report must contain sufficient data and rationale to support qualification of the FFS at the level requested.

d. As required by § 60.13(f), the sponsor must notify the responsible Flight Standards office when it becomes aware that an addition to, an amendment to, or a revision of data that may relate to FFS performance or handling characteristics is available. The data referred to in this paragraph is data used to validate the performance, handling qualities, or other characteristics of the aircraft, including data related to any relevant changes occurring after the type certificate was issued. The sponsor must-

(1) Within 10 calendar days, notify the responsible Flight Standards office of the existence of this data; and

(2) Within 45 calendar days, notify the responsible Flight Standards office of-

(a) The schedule to incorporate this data into the FFS; or

(b) The reason for not incorporating this data into the FFS.

e. In those cases where the objective test results authorize a "snapshot test" or a "series of snapshot tests" results in lieu of a time-history result, the sponsor or other data provider must ensure that a steady state condition exists at the instant of time captured by the "snapshot." The steady state condition must exist from 4 seconds prior to, through 1 second following, the instant of time captured by the snapshot.

f. The FFS sponsor is encouraged to maintain a liaison with the manufacturer of the aircraft being simulated (or with the holder of the aircraft type certificate for the aircraft being simulated if the manufacturer is no longer in business), and, if appropriate, with the person having supplied the aircraft data package for the FFS in order to facilitate the notification required by § 60.13(f).

g. It is the intent of the responsible Flight Standards office that for new aircraft entering service, at a point well in advance of preparation of the Qualification Test Guide (QTG), the sponsor should submit to the responsible Flight Standards office for approval, a descriptive document (see Table A2C, Sample Validation Data Roadmap for Airplanes) containing the plan for acquiring the validation data, including data sources. This document should clearly identify sources of data for all required tests, a description of the validity of these data for a specific engine type and thrust rating configuration, and the revision levels of all avionics affecting the performance or flying qualities of the aircraft. Additionally, this document should provide other information, such as the rationale or explanation for cases where data or data parameters are missing, instances where engineering simulation data are used or where flight test methods require further explanations. It should also provide a brief narrative describing the cause and effect of any deviation from data requirements. The aircraft manufacturer may provide this document.

h. There is no requirement for any flight test data supplier to submit a flight test plan or program prior to gathering flight test data. However, the responsible Flight Standards office notes that inexperienced data gatherers often provide data that is irrelevant, improperly marked, or lacking adequate justification for selection. Other problems include inadequate information regarding initial conditions or test maneuvers. The responsible Flight Standards office has been forced to refuse these data submissions as validation data for an FFS evaluation. It is for this reason that the responsible Flight Standards office recommends that any data supplier not previously experienced in this area review the data necessary for programming and for validating the performance of the FFS, and discuss the flight test plan anticipated for acquiring such data with the responsible Flight Standards office well in advance of commencing the flight tests.

i. The responsible Flight Standards office will consider, on a case-by-case basis, whether to approve supplemental validation data derived from flight data recording systems, such as a Quick Access Recorder or Flight Data Recorder.

a. In the event that the NSPM determines that special equipment or specifically qualified persons will be required to conduct an evaluation, the responsible Flight Standards office will make every attempt to notify the sponsor at least one (1) week, but in no case less than 72 hours, in advance of the evaluation. Examples of special equipment include spot photometers, flight control measurement devices, and sound analyzers. Examples of specially qualified personnel include individuals specifically qualified to install or use any special equipment when its use is required.

b. Examples of a special evaluation include an evaluation conducted after an FFS is moved, at the request of the TPAA, or as a result of comments received from users of the FFS that raise questions about the continued qualification or use of the FFS.

a. In order to be qualified at a particular qualification level, the FFS must:

(1) Meet the general requirements listed in Attachment 1 of this appendix;

(2) Meet the objective testing requirements listed in Attachment 2 of this appendix; and

(3) Satisfactorily accomplish the subjective tests listed in Attachment 3 of this appendix.

b. The request described in § 60.15(a) must include all of the following:

(1) A statement that the FFS meets all of the applicable provisions of this part and all applicable provisions of the QPS.

(2) Unless otherwise authorized through prior coordination with the responsible Flight Standards office, a confirmation that the sponsor will forward to the responsible Flight Standards office the statement described in § 60.15(b) in such time as to be received no later than 5 business days prior to the scheduled evaluation and may be forwarded to the responsible Flight Standards office via traditional or electronic means.

(3) A QTG, acceptable to the responsible Flight Standards office, that includes all of the following:

(a) Objective data obtained from traditional aircraft testing or another approved source.

(b) Correlating objective test results obtained from the performance of the FFS as prescribed in the appropriate QPS.

(c) The result of FFS subjective tests prescribed in the appropriate QPS.

(d) A description of the equipment necessary to perform the evaluation for initial qualification and the continuing qualification evaluations.

c. The QTG described in paragraph (a)(3) of this section, must provide the documented proof of compliance with the simulator objective tests in Attachment 2, Table A2A of this appendix.

d. The QTG is prepared and submitted by the sponsor, or the sponsor's agent on behalf of the sponsor, to the responsible Flight Standards office for review and approval, and must include, for each objective test:

(1) Parameters, tolerances, and flight conditions;

(2) Pertinent and complete instructions for the conduct of automatic and manual tests;

(3) A means of comparing the FFS test results to the objective data;

(4) Any other information as necessary, to assist in the evaluation of the test results;

(5) Other information appropriate to the qualification level of the FFS.

e. The QTG described in paragraphs (a)(3) and (b) of this section, must include the following:

(1) A QTG cover page with sponsor and FAA approval signature blocks (see Attachment 4, Figure A4C, of this appendix for a sample QTG cover page).

(2) [Reserved]

(3) An FFS information page that provides the information listed in this paragraph (see Attachment 4, Figure A4B, of this appendix for a sample FFS information page). For convertible FFSs, the sponsor must submit a separate page for each configuration of the FFS.

(a) The sponsor's FFS identification number or code.

(b) The airplane model and series being simulated.

(c) The aerodynamic data revision number or reference.

(d) The source of the basic aerodynamic model and the aerodynamic coefficient data used to modify the basic model.

(e) The engine model(s) and its data revision number or reference.

(f) The flight control data revision number or reference.

(g) The flight management system identification and revision level.

(h) The FFS model and manufacturer.

(i) The date of FFS manufacture.

(j) The FFS computer identification.

(k) The visual system model and manufacturer, including display type.

(l) The motion system type and manufacturer, including degrees of freedom.

(4) A Table of Contents.

(5) A log of revisions and a list of effective pages.

(6) A list of all relevant data references.

(7) A glossary of terms and symbols used (including sign conventions and units).

(8) Statements of Compliance and Capability (SOCs) with certain requirements.

(9) Recording procedures or equipment required to accomplish the objective tests.

(10) The following information for each objective test designated in Attachment 2, Table A2A, of this appendix as applicable to the qualification level sought:

(a) Name of the test.

(b) Objective of the test.

(c) Initial conditions.

(d) Manual test procedures.

(e) Automatic test procedures (if applicable).

(f) Method for evaluating FFS objective test results.

(g) List of all relevant parameters driven or constrained during the automatically conducted test(s).

(h) List of all relevant parameters driven or constrained during the manually conducted test(s).

(i) Tolerances for relevant parameters.

(j) Source of Validation Data (document and page number).

(k) Copy of the Validation Data (if located in a separate binder, a cross reference for the identification and page number for pertinent data location must be provided).

(l) Simulator Objective Test Results as obtained by the sponsor. Each test result must reflect the date completed and must be clearly labeled as a product of the device being tested.

f. A convertible FFS is addressed as a separate FFS for each model and series airplane to which it will be converted and for the FAA qualification level sought. If a sponsor seeks qualification for two or more models of an airplane type using a convertible FFS, the sponsor must submit a QTG for each airplane model, or a QTG for the first airplane model and a supplement to that QTG for each additional airplane model. The responsible Flight Standards officewill conduct evaluations for each airplane model.

g. Form and manner of presentation of objective test results in the QTG:

(1) The sponsor's FFS test results must be recorded in a manner acceptable to the responsible Flight Standards office, that allows easy comparison of the FFS test results to the validation data (e.g., use of a multi-channel recorder, line printer, cross plotting, overlays, transparencies).

(2) FFS results must be labeled using terminology common to airplane parameters as opposed to computer software identifications.

(3) Validation data documents included in a QTG may be photographically reduced only if such reduction will not alter the graphic scaling or cause difficulties in scale interpretation or resolution.

(4) Scaling on graphical presentations must provide the resolution necessary to evaluate the parameters shown in Attachment 2, Table A2A of this appendix.

(5) Tests involving time histories, data sheets (or transparencies thereof) and FFS test results must be clearly marked with appropriate reference points to ensure an accurate comparison between the FFS and the airplane with respect to time. Time histories recorded via a line printer are to be clearly identified for cross plotting on the airplane data. Over-plots must not obscure the reference data.

h. The sponsor may elect to complete the QTG objective and subjective tests at the manufacturer's facility or at the sponsor's training facility (or other sponsor designated location where training will take place). If the tests are conducted at the manufacturer's facility, the sponsor must repeat at least one-third of the tests at the sponsor's training facility in order to substantiate FFS performance. The QTG must be clearly annotated to indicate when and where each test was accomplished. Tests conducted at the manufacturer's facility and at the sponsor's designated training facility must be conducted after the FFS is assembled with systems and sub-systems functional and operating in an interactive manner. The test results must be submitted to the responsible Flight Standards office.

i. The sponsor must maintain a copy of the MQTG at the FFS location.

j. All FFSs for which the initial qualification is conducted after May 30, 2014, must have an electronic MQTG (eMQTG) including all objective data obtained from airplane testing, or another approved source (reformatted or digitized), together with correlating objective test results obtained from the performance of the FFS (reformatted or digitized) as prescribed in this appendix. The eMQTG must also contain the general FFS performance or demonstration results (reformatted or digitized) prescribed in this appendix, and a description of the equipment necessary to perform the initial qualification evaluation and the continuing qualification evaluations. The eMQTG must include the original validation data used to validate FFS performance and handling qualities in either the original digitized format from the data supplier or an electronic scan of the original time-history plots that were provided by the data supplier. A copy of the eMQTG must be provided to the responsible Flight Standards office.

k. All other FFSs not covered in subparagraph "j" must have an electronic copy of the MQTG by May 30, 2014. An electronic copy of the MQTG must be provided to the responsible Flight Standards office. This may be provided by an electronic scan presented in a Portable Document File (PDF), or similar format acceptable to the responsible Flight Standards office.

l. During the initial (or upgrade) qualification evaluation conducted by the responsible Flight Standards office, the sponsor must also provide a person who is a user of the device (e.g., a qualified pilot or instructor pilot with flight time experience in that aircraft) and knowledgeable about the operation of the aircraft and the operation of the FFS.

m. Only those FFSs that are sponsored by a certificate holder as defined in Appendix F of this part will be evaluated by the NSPM. However, other FFS evaluations may be conducted on a case-by-case basis as the Administrator deems appropriate, but only in accordance with applicable agreements.

n. The responsible Flight Standards office will conduct an evaluation for each configuration, and each FFS must be evaluated as completely as possible. To ensure a thorough and uniform evaluation, each FFS is subjected to the general simulator requirements in Attachment 1 of this appendix, the objective tests listed in Attachment 2 of this appendix, and the subjective tests listed in Attachment 3 of this appendix. The evaluations described herein will include, but not necessarily be limited to the following:

(1) Airplane responses, including longitudinal and lateral-directional control responses (see Attachment 2 of this appendix);

(2) Performance in authorized portions of the simulated airplane's operating envelope, to include tasks evaluated by the responsible Flight Standards office in the areas of surface operations, takeoff, climb, cruise, descent, approach, and landing as well as abnormal and emergency operations (see Attachment 2 of this appendix);

(3) Control checks (see Attachment 1 and Attachment 2 of this appendix);

(4) Flight deck configuration (see Attachment 1 of this appendix);

(5) Pilot, flight engineer, and instructor station functions checks (see Attachment 1 and Attachment 3 of this appendix);

(6) Airplane systems and sub-systems (as appropriate) as compared to the airplane simulated (see Attachment 1 and Attachment 3 of this appendix);

(7) FFS systems and sub-systems, including force cueing (motion), visual, and aural (sound) systems, as appropriate (see Attachment 1 and Attachment 2 of this appendix); and

(8) Certain additional requirements, depending upon the qualification level sought, including equipment or circumstances that may become hazardous to the occupants. The sponsor may be subject to Occupational Safety and Health Administration requirements.

o. The responsible Flight Standards office administers the objective and subjective tests, which includes an examination of functions. The tests include a qualitative assessment of the FFS by a pilot from the responsible Flight Standards office. The evaluation team leader may assign other qualified personnel to assist in accomplishing the functions examination and/or the objective and subjective tests performed during an evaluation when required.

(1) Objective tests provide a basis for measuring and evaluating FFS performance and determining compliance with the requirements of this part.

(2) Subjective tests provide a basis for:

(a) Evaluating the capability of the FFS to perform over a typical utilization period;

(b) Determining that the FFS satisfactorily simulates each required task;

(c) Verifying correct operation of the FFS controls, instruments, and systems; and

(d) Demonstrating compliance with the requirements of this part.

p. The tolerances for the test parameters listed in Attachment 2 of this appendix reflect the range of tolerances acceptable to the responsible Flight Standards office for FFS validation and are not to be confused with design tolerances specified for FFS manufacture. In making decisions regarding tests and test results, the responsible Flight Standards office relies on the use of operational and engineering judgment in the application of data (including consideration of the way in which the flight test was flown and the way the data was gathered and applied), data presentations, and the applicable tolerances for each test.

q. In addition to the scheduled continuing qualification evaluation, each FFS is subject to evaluations conducted by the responsible Flight Standards office at any time without prior notification to the sponsor. Such evaluations would be accomplished in a normal manner (i.e., requiring exclusive use of the FFS for the conduct of objective and subjective tests and an examination of functions) if the FFS is not being used for flight crewmember training, testing, or checking. However, if the FFS were being used, the evaluation would be conducted in a non-exclusive manner. This non-exclusive evaluation will be conducted by the FFS evaluator accompanying the check airman, instructor, Aircrew Program Designee (APD), or FAA inspector aboard the FFS along with the student(s) and observing the operation of the FFS during the training, testing, or checking activities.

r. Problems with objective test results are handled as follows:

(1) If a problem with an objective test result is detected by the evaluation team during an evaluation, the test may be repeated or the QTG may be amended.

(2) If it is determined that the results of an objective test do not support the level requested but do support a lower level, the responsible Flight Standards office may qualify the FFS at that lower level. For example, if a Level D evaluation is requested and the FFS fails to meet sound test tolerances, it could be qualified at Level C.

s. After an FFS is successfully evaluated, the responsible Flight Standards office issues a Statement of Qualification (SOQ) to the sponsor. The responsible Flight Standards office recommends the FFS to the TPAA, who will approve the FFS for use in a flight training program. The SOQ will be issued at the satisfactory conclusion of the initial or continuing qualification evaluation and will list the tasks for which the FFS is qualified, referencing the tasks described in Table A1B in Attachment 1 of this appendix. However, it is the sponsor's responsibility to obtain TPAA approval prior to using the FFS in an FAA-approved flight training program.

t. Under normal circumstances, the responsible Flight Standards office establishes a date for the initial or upgrade evaluation within ten (10) working days after determining that a complete QTG is acceptable. Unusual circumstances may warrant establishing an evaluation date before this determination is made. A sponsor may schedule an evaluation date as early as 6 months in advance. However, there may be a delay of 45 days or more in rescheduling and completing the evaluation if the sponsor is unable to meet the scheduled date. See Attachment 4 of this appendix, Figure A4A, Sample Request for Initial, Upgrade, or Reinstatement Evaluation.

u. The numbering system used for objective test results in the QTG should closely follow the numbering system set out in Attachment 2 of this appendix, FFS Objective Tests, Table A2A.

v. Contact the responsible Flight Standards office for additional information regarding the preferred qualifications of pilots used to meet the requirements of § 60.15(d).

w. Examples of the exclusions for which the FFS might not have been subjectively tested by the sponsor or the responsible Flight Standards office and for which qualification might not be sought or granted, as described in § 60.15(g)(6), include windshear training and circling approaches.

No additional regulatory or informational material applies to § 60.16, Additional Qualifications for a Currently Qualified FFS.

a. In instances where a sponsor plans to remove an FFS from active status for a period of less than two years, the following procedures apply:

(1) The responsible Flight Standards office must be notified in writing and the notification must include an estimate of the period that the FFS will be inactive;

(2) Continuing Qualification evaluations will not be scheduled during the inactive period;

(3) The responsible Flight Standards office will remove the FFS from the list of qualified FSTDs on a mutually established date not later than the date on which the first missed continuing qualification evaluation would have been scheduled;

(4) Before the FFS is restored to qualified status, it must be evaluated by the responsible Flight Standards office. The evaluation content and the time required to accomplish the evaluation is based on the number of continuing qualification evaluations and sponsor-conducted quarterly inspections missed during the period of inactivity.

(5) The sponsor must notify the responsible Flight Standards office of any changes to the original scheduled time out of service;

b. Simulators qualified prior to May 31, 2016, are not required to meet the general simulation requirements, the objective test requirements or the subjective test requirements of attachments 1, 2, and 3 of this appendix as long as the simulator continues to meet the test requirements contained in the MQTG developed under the original qualification basis.

c. After May 30, 2009, each visual scene or airport model beyond the minimum required for the FFS qualification level that is installed in and available for use in a qualified FFS must meet the requirements described in attachment 3 of this appendix.

d. Simulators qualified prior to May 31, 2016, may be updated. If an evaluation is deemed appropriate or necessary by the responsible Flight Standards office after such an update, the evaluation will not require an evaluation to standards beyond those against which the simulator was originally qualified.

e. Other certificate holders or persons desiring to use an FFS may contract with FFS sponsors to use FFSs previously qualified at a particular level for an airplane type and approved for use within an FAA-approved flight training program. Such FFSs are not required to undergo an additional qualification process, except as described in § 60.16.

f. Each FFS user must obtain approval from the appropriate TPAA to use any FFS in an FAA-approved flight training program.

g. The intent of the requirement listed in § 60.17(b), for each FFS to have a SOQ within 6 years, is to have the availability of that statement (including the configuration list and the limitations to authorizations) to provide a complete picture of the FFS inventory regulated by the FAA. The issuance of the statement will not require any additional evaluation or require any adjustment to the evaluation basis for the FFS.

h. Downgrading of an FFS is a permanent change in qualification level and will necessitate the issuance of a revised SOQ to reflect the revised qualification level, as appropriate. If a temporary restriction is placed on an FFS because of a missing, malfunctioning, or inoperative component or on-going repairs, the restriction is not a permanent change in qualification level. Instead, the restriction is temporary and is removed when the reason for the restriction has been resolved.

i. The responsible Flight Standards office will determine the evaluation criteria for an FFS that has been removed from active status. The criteria will be based on the number of continuing qualification evaluations and quarterly inspections missed during the period of inactivity. For example, if the FFS were out of service for a 1 year period, it would be necessary to complete the entire QTG, since all of the quarterly evaluations would have been missed. The responsible Flight Standards office will also consider how the FFS was stored, whether parts were removed from the FFS and whether the FFS was disassembled.

j. The FFS will normally be requalified using the FAA-approved MQTG and the criteria that was in effect prior to its removal from qualification. However, inactive periods of 2 years or more will require requalification under the standards in effect and current at the time of requalification.

a. The sponsor must conduct a minimum of four evenly spaced inspections throughout the year. The objective test sequence and content of each inspection must be developed by the sponsor and must be acceptable to the responsible Flight Standards office.

b. The description of the functional preflight check must be contained in the sponsor's QMS.

c. Record "functional preflight" in the FFS discrepancy log book or other acceptable location, including any item found to be missing, malfunctioning, or inoperative.

d. During the continuing qualification evaluation conducted by the responsible Flight Standards office, the sponsor must also provide a person knowledgeable about the operation of the aircraft and the operation of the FFS.

e. The responsible Flight Standards office will conduct continuing qualification evaluations every 12 months unless:

(1) The responsible Flight Standards office becomes aware of discrepancies or performance problems with the device that warrants more frequent evaluations; or

(2) The sponsor implements a QMS that justifies less frequent evaluations. However, in no case shall the frequency of a continuing qualification evaluation exceed 36 months.

f. The sponsor's test sequence and the content of each quarterly inspection required in § 60.19(a)(1) should include a balance and a mix from the objective test requirement areas listed as follows:

(1) Performance.

(2) Handling qualities.

(3) Motion system (where appropriate).

(4) Visual system (where appropriate).

(5) Sound system (where appropriate).

(6) Other FFS systems.

g. If the evaluator plans to accomplish specific tests during a normal continuing qualification evaluation that requires the use of special equipment or technicians, the sponsor will be notified as far in advance of the evaluation as practical; but not less than 72 hours. Examples of such tests include latencies, control dynamics, sounds and vibrations, motion, and/or some visual system tests.

h. The continuing qualification evaluations, described in § 60.19(b), will normally require 4 hours of FFS time. However, flexibility is necessary to address abnormal situations or situations involving aircraft with additional levels of complexity (e.g., computer controlled aircraft). The sponsor should anticipate that some tests may require additional time. The continuing qualification evaluations will consist of the following:

(1) Review of the results of the quarterly inspections conducted by the sponsor since the last scheduled continuing qualification evaluation.

(2) A selection of approximately 8 to 15 objective tests from the MQTG that provide an adequate opportunity to evaluate the performance of the FFS. The tests chosen will be performed either automatically or manually and should be able to be conducted within approximately one-third () of the allotted FFS time.

(3) A subjective evaluation of the FFS to perform a representative sampling of the tasks set out in attachment 3 of this appendix. This portion of the evaluation should take approximately two-thirds () of the allotted FFS time.

(4) An examination of the functions of the FFS may include the motion system, visual system, sound system, instructor operating station, and the normal functions and simulated malfunctions of the airplane systems. This examination is normally accomplished simultaneously with the subjective evaluation requirements.

No additional regulatory or informational material applies to § 60.20. Logging FFS Discrepancies.

No additional regulatory or informational material applies to § 60.21, Interim Qualification of FFSs for New Airplane Types or Models.

a. The notification described in § 60.23(c)(2) must include a complete description of the planned modification, with a description of the operational and engineering effect the proposed modification will have on the operation of the FFS and the results that are expected with the modification incorporated.

b. Prior to using the modified FFS:

(1) All the applicable objective tests completed with the modification incorporated, including any necessary updates to the MQTG (e.g., accomplishment of FSTD Directives) must be acceptable to the responsible Flight Standards office; and

(2) The sponsor must provide the responsible Flight Standards office with a statement signed by the MR that the factors listed in § 60.15(b) are addressed by the appropriate personnel as described in that section.

FSTD Directives are considered modifications of an FFS. See Attachment 4 of this appendix for a sample index of effective FSTD Directives. See Attachment 6 of this appendix for a list of all effective FSTD Directives applicable to Airplane FFSs.

a. The sponsor's responsibility with respect to § 60.25(a) is satisfied when the sponsor fairly and accurately advises the user of the current status of an FFS, including any missing, malfunctioning, or inoperative (MMI) component(s).

b. It is the responsibility of the instructor, check airman, or representative of the administrator conducting training, testing, or checking to exercise reasonable and prudent judgment to determine if any MMI component is necessary for the satisfactory completion of a specific maneuver, procedure, or task.

c. If the 29th or 30th day of the 30-day period described in § 60.25(b) is on a Saturday, a Sunday, or a holiday, the FAA will extend the deadline until the next business day.

d. In accordance with the authorization described in § 60.25(b), the sponsor may develop a discrepancy prioritizing system to accomplish repairs based on the level of impact on the capability of the FFS. Repairs having a larger impact on FFS capability to provide the required training, evaluation, or flight experience will have a higher priority for repair or replacement.

If the sponsor provides a plan for how the FFS will be maintained during its out-of-service period (e.g., periodic exercise of mechanical, hydraulic, and electrical systems; routine replacement of hydraulic fluid; control of the environmental factors in which the FFS is to be maintained) there is a greater likelihood that the responsible Flight Standards office will be able to determine the amount of testing required for requalification.

If the sponsor provides a plan for how the FFS will be maintained during its out-of-service period (e.g., periodic exercise of mechanical, hydraulic, and electrical systems; routine replacement of hydraulic fluid; control of the environmental factors in which the FFS is to be maintained) there is a greater likelihood that the responsible Flight Standards office will be able to determine the amount of testing required for requalification.

a. FFS modifications can include hardware or software changes. For FFS modifications involving software programming changes, the record required by § 60.31(a)(2) must consist of the name of the aircraft system software, aerodynamic model, or engine model change, the date of the change, a summary of the change, and the reason for the change.

b. If a coded form for record keeping is used, it must provide for the preservation and retrieval of information with appropriate security or controls to prevent the inappropriate alteration of such records after the fact.

No additional regulatory or informational material applies to § 60.33, Applications, Logbooks, Reports, and Records: Fraud, Falsification, or Incorrect Statements.

No additional regulatory or informational material applies to § 60.35, Specific FFS Compliance Requirements.

No additional regulatory or informational material applies to § 60.37, FFS Qualification on the Basis of a Bilateral Aviation Safety Agreement (BASA).

a. Certain requirements included in this appendix must be supported with an SOC as defined in Appendix F, which may include objective and subjective tests. The requirements for SOCs are indicated in the "General Simulator Requirements" column in Table A1A of this appendix.

b. Table A1A describes the requirements for the indicated level of FFS. Many devices include operational systems or functions that exceed the requirements outlined in this section. However, all systems will be tested and evaluated in accordance with this appendix to ensure proper operation.

a. This attachment describes the general simulator requirements for qualifying an airplane FFS. The sponsor should also consult the objective tests in Attachment 2 of this appendix and the examination of functions and subjective tests listed in Attachment 3 of this appendix to determine the complete requirements for a specific level simulator.

b. The material contained in this attachment is divided into the following categories:

(1) General flight deck configuration.

(2) Simulator programming.

(3) Equipment operation.

(4) Equipment and facilities for instructor/evaluator functions.

(5) Motion system.

(6) Visual system.

(7) Sound system.

c. Table A1A provides the standards for the General Simulator Requirements.

d. Table A1B provides the tasks that the sponsor will examine to determine whether the FFS satisfactorily meets the requirements for flight crew training, testing, and experience, and provides the tasks for which the simulator may be qualified.

e. Table A1C provides the functions that an instructor/check airman must be able to control in the simulator.

f. It is not required that all of the tasks that appear on the List of Qualified Tasks (part of the SOQ) be accomplished during the initial or continuing qualification evaluation.

a. For the purposes of this attachment, the flight conditions specified in the Flight Conditions Column of Table A2A of this appendix, are defined as follows:

(1) Ground-on ground, independent of airplane configuration;

(2) Take-off-gear down with flaps/slats in any certified takeoff position;

(3) First segment climb-gear down with flaps/slats in any certified takeoff position (normally not above 50 ft AGL);

(4) Second segment climb-gear up with flaps/slats in any certified takeoff position (normally between 50 ft and 400 ft AGL);

(5) Clean-flaps/slats retracted and gear up;

(6) Cruise-clean configuration at cruise altitude and airspeed;

(7) Approach-gear up or down with flaps/slats at any normal approach position as recommended by the airplane manufacturer; and

(8) Landing-gear down with flaps/slats in any certified landing position.

b. The format for numbering the objective tests in Appendix A, Attachment 2, Table A2A, and the objective tests in Appendix B, Attachment 2, Table B2A, is identical. However, each test required for FFSs is not necessarily required for FTDs. Also, each test required for FTDs is not necessarily required for FFSs. Therefore, when a test number (or series of numbers) is not required, the term "Reserved" is used in the table at that location. Following this numbering format provides a degree of commonality between the two tables and substantially reduces the potential for confusion when referring to objective test numbers for either FFSs or FTDs.

c. The reader is encouraged to review the Airplane Flight Simulator Evaluation Handbook, Volumes I and II, published by the Royal Aeronautical Society, London, UK, and AC 25-7, as amended, Flight Test Guide for Certification of Transport Category Airplanes, and AC 23-8, as amended, Flight Test Guide for Certification of Part 23 Airplanes, for references and examples regarding flight testing requirements and techniques.

d. If relevant winds are present in the objective data, the wind vector should be clearly noted as part of the data presentation, expressed in conventional terminology, and related to the runway being used for the test.

a. The ground and flight tests required for qualification are listed in Table A2A, FFS Objective Tests. Computer generated simulator test results must be provided for each test except where an alternative test is specifically authorized by the responsible Flight Standards office. If a flight condition or operating condition is required for the test but does not apply to the airplane being simulated or to the qualification level sought, it may be disregarded (e.g., an engine out missed approach for a single-engine airplane or a maneuver using reverse thrust for an airplane without reverse thrust capability). Each test result is compared against the validation data described in § 60.13 and in this appendix. Although use of a driver program designed to automatically accomplish the tests is encouraged for all simulators and required for Level C and Level D simulators, it must be possible to conduct each test manually while recording all appropriate parameters. The results must be produced on an appropriate recording device acceptable to the responsible Flight Standards office and must include simulator number, date, time, conditions, tolerances, and appropriate dependent variables portrayed in comparison to the validation data. Time histories are required unless otherwise indicated in Table A2A. All results must be labeled using the tolerances and units given.

b. Table A2A in this attachment sets out the test results required, including the parameters, tolerances, and flight conditions for simulator validation. Tolerances are provided for the listed tests because mathematical modeling and acquisition and development of reference data are often inexact. All tolerances listed in the following tables are applied to simulator performance. When two tolerance values are given for a parameter, the less restrictive may be used unless otherwise indicated. In those cases where a tolerance is expressed only as a percentage, the tolerance percentage applies to the maximum value of that parameter within its normal operating range as measured from the neutral or zero position unless otherwise indicated.

c. Certain tests included in this attachment must be supported with an SOC. In Table A2A, requirements for SOCs are indicated in the "Test Details" column.

d. When operational or engineering judgment is used in making assessments for flight test data applications for simulator validity, such judgment must not be limited to a single parameter. For example, data that exhibit rapid variations of the measured parameters may require interpolations or a "best fit" data selection. All relevant parameters related to a given maneuver or flight condition must be provided to allow overall interpretation. When it is difficult or impossible to match simulator to airplane data throughout a time history, differences must be justified by providing a comparison of other related variables for the condition being assessed.

e. It is not acceptable to program the FFS so that the mathematical modeling is correct only at the validation test points. Unless otherwise noted, simulator tests must represent airplane performance and handling qualities at operating weights and centers of gravity (CG) typical of normal operation. Simulator tests at extreme weight or CG conditions may be acceptable where required for concurrent aircraft certification testing. Tests of handling qualities must include validation of augmentation devices.

f. When comparing the parameters listed to those of the airplane, sufficient data must also be provided to verify the correct flight condition and airplane configuration changes. For example, to show that control force is within the parameters for a static stability test, data to show the correct airspeed, power, thrust or torque, airplane configuration, altitude, and other appropriate datum identification parameters must also be given. If comparing short period dynamics, normal acceleration may be used to establish a match to the airplane, but airspeed, altitude, control input, airplane configuration, and other appropriate data must also be given. If comparing landing gear change dynamics, pitch, airspeed, and altitude may be used to establish a match to the airplane, but landing gear position must also be provided. All airspeed values must be properly annotated (e.g., indicated versus calibrated). In addition, the same variables must be used for comparison (e.g., compare inches to inches rather than inches to centimeters).

g. The QTG provided by the sponsor must clearly describe how the simulator will be set up and operated for each test. Each simulator subsystem may be tested independently, but overall integrated testing of the simulator must be accomplished to assure that the total simulator system meets the prescribed standards. A manual test procedure with explicit and detailed steps for completing each test must also be provided.

h. For previously qualified simulators, the tests and tolerances of this attachment may be used in subsequent continuing qualification evaluations for any given test if the sponsor has submitted a proposed MQTG revision to the responsible Flight Standards office and has received responsible Flight Standards office approval.

i. Simulators are evaluated and qualified with an engine model simulating the airplane data supplier's flight test engine. For qualification of alternative engine models (either variations of the flight test engines or other manufacturer's engines) additional tests with the alternative engine models may be required. This attachment contains guidelines for alternative engines.

j. For testing Computer Controlled Aircraft (CCA) simulators, or other highly augmented airplane simulators, flight test data is required for the Normal (N) and/or Non-normal (NN) control states, as indicated in this attachment. Where test results are independent of control state, Normal or Non-normal control data may be used. All tests in Table A2A require test results in the Normal control state unless specifically noted otherwise in the Test Details section following the CCA designation. The responsible Flight Standards office will determine what tests are appropriate for airplane simulation data. When making this determination, the responsible Flight Standards office may require other levels of control state degradation for specific airplane tests. Where Non-normal control states are required, test data must be provided for one or more Non-normal control states, and must include the least augmented state. Where applicable, flight test data must record Normal and Non-normal states for:

(1) Pilot controller deflections or electronically generated inputs, including location of input; and

(2) Flight control surface positions unless test results are not affected by, or are independent of, surface positions.

k. Tests of handling qualities must include validation of augmentation devices. FFSs for highly augmented airplanes will be validated both in the unaugmented configuration (or failure state with the maximum permitted degradation in handling qualities) and the augmented configuration. Where various levels of handling qualities result from failure states, validation of the effect of the failure is necessary. Requirements for testing will be mutually agreed to between the sponsor and the responsible Flight Standards office on a case-by-case basis.

l. Some tests will not be required for airplanes using airplane hardware in the simulator flight deck (e.g., "side stick controller"). These exceptions are noted in Section 2 "Handling Qualities" in Table A2A of this attachment. However, in these cases, the sponsor must provide a statement that the airplane hardware meets the appropriate manufacturer's specifications and the sponsor must have supporting information to that fact available for responsible Flight Standards office review.

m. For objective test purposes, see Appendix F of this part for the definitions of "Near maximum," "Light," and "Medium" gross weight.

n. In those cases where the objective test results authorize a "snapshot test" or a "series of snapshot tests" results in lieu of a time-history result, the sponsor or other data provider must ensure that a steady state condition exists at the instant of time captured by the "snapshot." The steady state condition should exist from 4 seconds prior to, through 1 second following, the instant of time captured by the snap shot.

o. For references on basic operating weight, see AC 120-27, "Aircraft Weight and Balance;" and FAA-H-8083-1, "Aircraft Weight and Balance Handbook."

a. If relevant winds are present in the objective data, the wind vector should be clearly noted as part of the data presentation, expressed in conventional terminology, and related to the runway being used for test near the ground.

b. The reader is encouraged to review the Airplane Flight Simulator Evaluation Handbook, Volumes I and II, published by the Royal Aeronautical Society, London, UK, and AC 25-7, as amended, Flight Test Guide for Certification of Transport Category Airplanes, and AC 23-8, as amended, Flight Test Guide for Certification of Part 23 Airplanes, for references and examples regarding flight testing requirements and techniques.

a. General. The characteristics of an airplane flight control system have a major effect on handling qualities. A significant consideration in pilot acceptability of an airplane is the "feel" provided through the flight controls. Considerable effort is expended on airplane feel system design so that pilots will be comfortable and will consider the airplane desirable to fly. In order for an FFS to be representative, it should "feel" like the airplane being simulated. Compliance with this requirement is determined by comparing a recording of the control feel dynamics of the FFS to actual airplane measurements in the takeoff, cruise and landing configurations.

(1) Recordings such as free response to an impulse or step function are classically used to estimate the dynamic properties of electromechanical systems. In any case, it is only possible to estimate the dynamic properties as a result of being able to estimate true inputs and responses. Therefore, it is imperative that the best possible data be collected since close matching of the FFS control loading system to the airplane system is essential. The required dynamic control tests are described in Table A2A of this attachment.

(2) For initial and upgrade evaluations, the QPS requires that control dynamics characteristics be measured and recorded directly from the flight controls (Handling Qualities-Table A2A). This procedure is usually accomplished by measuring the free response of the controls using a step or impulse input to excite the system. The procedure should be accomplished in the takeoff, cruise and landing flight conditions and configurations.

(3) For airplanes with irreversible control systems, measurements may be obtained on the ground if proper pitot-static inputs are provided to represent airspeeds typical of those encountered in flight. Likewise, it may be shown that for some airplanes, takeoff, cruise, and landing configurations have like effects. Thus, one may suffice for another. In either case, engineering validation or airplane manufacturer rationale should be submitted as justification for ground tests or for eliminating a configuration. For FFSs requiring static and dynamic tests at the controls, special test fixtures will not be required during initial and upgrade evaluations if the QTG shows both test fixture results and the results of an alternate approach (e.g., computer plots that were produced concurrently and show satisfactory agreement). Repeat of the alternate method during the initial evaluation satisfies this test requirement.

b. Control Dynamics Evaluation. The dynamic properties of control systems are often stated in terms of frequency, damping and a number of other classical measurements. In order to establish a consistent means of validating test results for FFS control loading, criteria are needed that will clearly define the measurement interpretation and the applied tolerances. Criteria are needed for underdamped, critically damped and overdamped systems. In the case of an underdamped system with very light damping, the system may be quantified in terms of frequency and damping. In critically damped or overdamped systems, the frequency and damping are not readily measured from a response time history. Therefore, the following suggested measurements may be used:

(1) For Level C and D simulators. Tests to verify that control feel dynamics represent the airplane should show that the dynamic damping cycles (free response of the controls) match those of the airplane within specified tolerances. The Flight Standards Service recognizes that several different testing methods may be used to verify the control feel dynamic response. The responsible Flight Standards office will consider the merits of testing methods based on reliability and consistency. One acceptable method of evaluating the response and the tolerance to be applied is described below for the underdamped and critically damped cases. A sponsor using this method to comply with the QPS requirements should perform the tests as follows:

(a) Underdamped response. Two measurements are required for the period, the time to first zero crossing (in case a rate limit is present) and the subsequent frequency of oscillation. It is necessary to measure cycles on an individual basis in case there are non-uniform periods in the response. Each period will be independently compared to the respective period of the airplane control system and, consequently, will enjoy the full tolerance specified for that period. The damping tolerance will be applied to overshoots on an individual basis. Care should be taken when applying the tolerance to small overshoots since the significance of such overshoots becomes questionable. Only those overshoots larger than 5 per cent of the total initial displacement should be considered. The residual band, labeled T(A) on Figure A2A is +/-5 percent of the initial displacement amplitude Afrom the steady state value of the oscillation. Only oscillations outside the residual band are considered significant. When comparing FFS data to airplane data, the process should begin by overlaying or aligning the FFS and airplane steady state values and then comparing amplitudes of oscillation peaks, the time of the first zero crossing and individual periods of oscillation. The FFS should show the same number of significant overshoots to within one when compared against the airplane data. The procedure for evaluating the response is illustrated in Figure A2A.

(b) Critically damped and overdamped response. Due to the nature of critically damped and overdamped responses (no overshoots), the time to reach 90 percent of the steady state (neutral point) value should be the same as the airplane within +/-10 percent. Figure A2B illustrates the procedure.

(c) Special considerations. Control systems that exhibit characteristics other than classical overdamped or underdamped responses should meet specified tolerances. In addition, special consideration should be given to ensure that significant trends are maintained.

(2) Tolerances.

(a) The following table summarizes the tolerances, T, for underdamped systems, and "n" is the sequential period of a full cycle of oscillation. See Figure A2A of this attachment for an illustration of the referenced measurements.

Significant overshoots, First overshoot and +/-1 subsequent overshoots.

(b) The following tolerance applies to critically damped and overdamped systems only. See Figure A2B for an illustration of the reference measurements:

c. Alternative method for control dynamics evaluation.

(1) An alternative means for validating control dynamics for aircraft with hydraulically powered flight controls and artificial feel systems is by the measurement of control force and rate of movement. For each axis of pitch, roll, and yaw, the control must be forced to its maximum extreme position for the following distinct rates. These tests are conducted under normal flight and ground conditions.

(a) Static test-Slowly move the control so that a full sweep is achieved within 95 to 105 seconds. A full sweep is defined as movement of the controller from neutral to the stop, usually aft or right stop, then to the opposite stop, then to the neutral position.

(b) Slow dynamic test-Achieve a full sweep within 8-12 seconds.

(c) Fast dynamic test-Achieve a full sweep within 3-5 seconds.

(d) Tolerances

(i) Static test; see Table A2A, FFS Objective Tests, Entries 2.a.1., 2.a.2., and 2.a.3.

(ii) Dynamic test-+/-2 lbs (0.9 daN) or +/-10% on dynamic increment above static test.

d. The FAA is open to alternative means such as the one described above. The alternatives should be justified and appropriate to the application. For example, the method described here may not apply to all manufacturers' systems and certainly not to aircraft with reversible control systems. Each case is considered on its own merit on an ad hoc basis. If the FAA finds that alternative methods do not result in satisfactory performance, more conventionally accepted methods will have to be used.

a. For an FFS to be used for take-off and landing (not applicable to Level A simulators in that the landing maneuver may not be credited in a Level A simulator) it should reproduce the aerodynamic changes that occur in ground effect. The parameters chosen for FFS validation should indicate these changes.

(1) A dedicated test should be provided that will validate the aerodynamic ground effect characteristics.

(2) The organization performing the flight tests may select appropriate test methods and procedures to validate ground effect. However, the flight tests should be performed with enough duration near the ground to sufficiently validate the ground-effect model.

b. The responsible Flight Standards office will consider the merits of testing methods based on reliability and consistency. Acceptable methods of validating ground effect are described below. If other methods are proposed, rationale should be provided to conclude that the tests performed validate the ground-effect model. A sponsor using the methods described below to comply with the QPS requirements should perform the tests as follows:

(1) Level fly-bys. The level fly-bys should be conducted at a minimum of three altitudes within the ground effect, including one at no more than 10% of the wingspan above the ground, one each at approximately 30% and 50% of the wingspan where height refers to main gear tire above the ground. In addition, one level-flight trim condition should be conducted out of ground effect (e.g., at 150% of wingspan).

(2) Shallow approach landing. The shallow approach landing should be performed at a glide slope of approximately one degree with negligible pilot activity until flare.

c. The lateral-directional characteristics are also altered by ground effect. For example, because of changes in lift, roll damping is affected. The change in roll damping will affect other dynamic modes usually evaluated for FFS validation. In fact, Dutch roll dynamics, spiral stability, and roll-rate for a given lateral control input are altered by ground effect. Steady heading sideslips will also be affected. These effects should be accounted for in the FFS modeling. Several tests such as crosswind landing, one engine inoperative landing, and engine failure on take-off serve to validate lateral-directional ground effect since portions of these tests are accomplished as the aircraft is descending through heights above the runway at which ground effect is an important factor.

a. General.

(1) Pilots use continuous information signals to regulate the state of the airplane. In concert with the instruments and outside-world visual information, whole-body motion feedback is essential in assisting the pilot to control the airplane dynamics, particularly in the presence of external disturbances. The motion system should meet basic objective performance criteria, and should be subjectively tuned at the pilot's seat position to represent the linear and angular accelerations of the airplane during a prescribed minimum set of maneuvers and conditions. The response of the motion cueing system should also be repeatable.

(2) The Motion System tests in Section 3 of Table A2A are intended to qualify the FFS motion cueing system from a mechanical performance standpoint. Additionally, the list of motion effects provides a representative sample of dynamic conditions that should be present in the flight simulator. An additional list of representative, training-critical maneuvers, selected from Section 1 (Performance tests), and Section 2 (Handling Qualities tests), in Table A2A, that should be recorded during initial qualification (but without tolerance) to indicate the flight simulator motion cueing performance signature have been identified (reference Section 3.e). These tests are intended to help improve the overall standard of FFS motion cueing.

b. Motion System Checks. The intent of test 3a, Frequency Response, and test 3b, Turn-Around Check, as described in the Table of Objective Tests, are to demonstrate the performance of the motion system hardware, and to check the integrity of the motion set-up with regard to calibration and wear. These tests are independent of the motion cueing software and should be considered robotic tests.

c. Motion System Repeatability. The intent of this test is to ensure that the motion system software and motion system hardware have not degraded or changed over time. This diagnostic test should be completed during continuing qualification checks in lieu of the robotic tests. This will allow an improved ability to determine changes in the software or determine degradation in the hardware. The following information delineates the methodology that should be used for this test.

(1) Input: The inputs should be such that rotational accelerations, rotational rates, and linear accelerations are inserted before the transfer from airplane center of gravity to pilot reference point with a minimum amplitude of 5 deg/sec/sec, 10 deg/sec and 0.3 g, respectively, to provide adequate analysis of the output.

(2) Recommended output:

(a) Actual platform linear accelerations; the output will comprise accelerations due to both the linear and rotational motion acceleration;

(b) Motion actuators position.

d. Objective Motion Cueing Test-Frequency Domain

(1) Background. This test quantifies the response of the motion cueing system from the output of the flight model to the motion platform response. Other motion tests, such as the motion system frequency response, concentrate on the mechanical performance of the motion system hardware alone. The intent of this test is to provide quantitative frequency response records of the entire motion system for specified degree-of-freedom transfer relationships over a range of frequencies. This range should be representative of the manual control range for that particular aircraft type and the simulator as set up during qualification. The measurements of this test should include the combined influence of the motion cueing algorithm, the motion platform dynamics, and the transport delay associated with the motion cueing and control system implementation. Specified frequency responses describing the ability of the FSTD to reproduce aircraft translations and rotations, as well as the cross-coupling relations, are required as part of these measurements. When simulating forward aircraft acceleration, the simulator is accelerated momentarily in the forward direction to provide the onset cueing. This is considered the direct transfer relation. The simulator is simultaneously tilted nose-up due to the low-pass filter in order to generate a sustained specific force. The tilt associated with the generation of the sustained specific force, and the angular rates and angular accelerations associated with the initiation of the sustained specific force, are considered cross-coupling relations. The specific force is required for the perception of the aircraft sustained specific force, while the angular rates and accelerations do not occur in the aircraft and should be minimized.

(2) Frequency response test. This test requires the frequency response to be measured for the motion cueing system. Reference sinusoidal signals are inserted at the pilot reference position prior to the motion cueing computations. The response of the motion platform in the corresponding degree-of-freedom (the direct transfer relations), as well as the motions resulting from cross-coupling (the cross-coupling relations), are recorded. These are the tests that are important to pilot motion cueing and are general tests applicable to all types of airplanes.

(3) This test is only required to be run once for the initial qualification of the FSTD and will not be required for continuing qualification purposes. The FAA will accept test results provided by the FSTD manufacturer as part of a Statement of Compliance confirming that the objective motion cueing tests were used to assist in the tuning of the FSTD's motion cueing algorithms.

e. Motion Vibrations.

(1) Presentation of results. The characteristic motion vibrations may be used to verify that the flight simulator can reproduce the frequency content of the airplane when flown in specific conditions. The test results should be presented as a Power Spectral Density (PSD) plot with frequencies on the horizontal axis and amplitude on the vertical axis. The airplane data and flight simulator data should be presented in the same format with the same scaling. The algorithms used for generating the flight simulator data should be the same as those used for the airplane data. If they are not the same then the algorithms used for the flight simulator data should be proven to be sufficiently comparable. As a minimum, the results along the dominant axes should be presented and a rationale for not presenting the other axes should be provided.

(2) Interpretation of results. The overall trend of the PSD plot should be considered while focusing on the dominant frequencies. Less emphasis should be placed on the differences at the high frequency and low amplitude portions of the PSD plot. During the analysis, certain structural components of the flight simulator have resonant frequencies that are filtered and may not appear in the PSD plot. If filtering is required, the notch filter bandwidth should be limited to 1 Hz to ensure that the buffet feel is not adversely affected. In addition, a rationale should be provided to explain that the characteristic motion vibration is not being adversely affected by the filtering. The amplitude should match airplane data as described below. However, if the PSD plot was altered for subjective reasons, a rationale should be provided to justify the change. If the plot is on a logarithmic scale, it may be difficult to interpret the amplitude of the buffet in terms of acceleration. For example, a 1 x 10g-rms/Hz would describe a heavy buffet and may be seen in the deep stall regime. Alternatively, a 1 x 10g-rms/Hz buffet is almost not perceivable; but may represent a flap buffet at low speed. The previous two examples differ in magnitude by 1000. On a PSD plot this represents three decades (one decade is a change in order of magnitude of 10; and two decades is a change in order of magnitude of 100).

a. General. The total sound environment in the airplane is very complex, and changes with atmospheric conditions, airplane configuration, airspeed, altitude, and power settings. Flight deck sounds are an important component of the flight deck operational environment and provide valuable information to the flight crew. These aural cues can either assist the crew (as an indication of an abnormal situation), or hinder the crew (as a distraction or nuisance). For effective training, the flight simulator should provide flight deck sounds that are perceptible to the pilot during normal and abnormal operations, and comparable to those of the airplane. The flight simulator operator should carefully evaluate background noises in the location where the device will be installed. To demonstrate compliance with the sound requirements, the objective or validation tests in this attachment were selected to provide a representative sample of normal static conditions typically experienced by a pilot.

b. Alternate propulsion. For FFS with multiple propulsion configurations, any condition listed in Table A2A of this attachment should be presented for evaluation as part of the QTG if identified by the airplane manufacturer or other data supplier as significantly different due to a change in propulsion system (engine or propeller).

c. Data and Data Collection System.

(1) Information provided to the flight simulator manufacturer should be presented in the format suggested by the International Air Transport Association (IATA) "Flight Simulator Design and Performance Data Requirements," as amended. This information should contain calibration and frequency response data.

(2) The system used to perform the tests listed in Table A2A should comply with the following standards:

(a) The specifications for octave, half octave, and third octave band filter sets may be found in American National Standards Institute (ANSI) S1.11-1986;

(b) Measurement microphones should be type WS2 or better, as described in International Electrotechnical Commission (IEC) 1094-4-1995.

(3) Headsets. If headsets are used during normal operation of the airplane they should also be used during the flight simulator evaluation.

(4) Playback equipment. Playback equipment and recordings of the QTG conditions should be provided during initial evaluations.

(5) Background noise.

(a) Background noise is the noise in the flight simulator that is not associated with the airplane, but is caused by the flight simulator's cooling and hydraulic systems and extraneous noise from other locations in the building. Background noise can seriously impact the correct simulation of airplane sounds and should be kept below the airplane sounds. In some cases, the sound level of the simulation can be increased to compensate for the background noise. However, this approach is limited by the specified tolerances and by the subjective acceptability of the sound environment to the evaluation pilot.

(b) The acceptability of the background noise levels is dependent upon the normal sound levels in the airplane being represented. Background noise levels that fall below the lines defined by the following points, may be acceptable:

(i) 70 dB @ 50 Hz;

(ii) 55 dB @ 1000 Hz;

(iii) 30 dB @ 16 kHz

(These limits are for unweightedoctave band sound levels. Meeting these limits for background noise does not ensure an acceptable flight simulator. Airplane sounds that fall below this limit require careful review and may require lower limits on background noise.)

(6) Validation testing. Deficiencies in airplane recordings should be considered when applying the specified tolerances to ensure that the simulation is representative of the airplane. Examples of typical deficiencies are:

(a) Variation of data between tail numbers;

(b) Frequency response of microphones;

(c) Repeatability of the measurements.

a. Typically, an airplane manufacturer's approved final data for performance, handling qualities, systems or avionics is not available until well after a new or derivative airplane has entered service. However, flight crew training and certification often begins several months prior to the entry of the first airplane into service. Consequently, it may be necessary to use preliminary data provided by the airplane manufacturer for interim qualification of flight simulators.

b. In these cases, the responsible Flight Standards office may accept certain partially validated preliminary airplane and systems data, and early release ("red label") avionics data in order to permit the necessary program schedule for training, certification, and service introduction.

c. Simulator sponsors seeking qualification based on preliminary data should consult the responsible Flight Standards office to make special arrangements for using preliminary data for flight simulator qualification. The sponsor should also consult the airplane and flight simulator manufacturers to develop a data plan and flight simulator qualification plan.

d. The procedure to be followed to gain the responsible Flight Standards office acceptance of preliminary data will vary from case to case and between airplane manufacturers. Each airplane manufacturer's new airplane development and test program is designed to suit the needs of the particular project and may not contain the same events or sequence of events as another manufacturer's program, or even the same manufacturer's program for a different airplane. Therefore, there cannot be a prescribed invariable procedure for acceptance of preliminary data, but instead there should be a statement describing the final sequence of events, data sources, and validation procedures agreed by the simulator sponsor, the airplane manufacturer, the flight simulator manufacturer, and the responsible Flight Standards office.

e. The preliminary data should be the manufacturer's best representation of the airplane, with assurance that the final data will not significantly deviate from the preliminary estimates. Data derived from these predictive or preliminary techniques should be validated against available sources including, at least, the following:

(1) Manufacturer's engineering report. The report should explain the predictive method used and illustrate past success of the method on similar projects. For example, the manufacturer could show the application of the method to an earlier airplane model or predict the characteristics of an earlier model and compare the results to final data for that model.

(2) Early flight test results. This data is often derived from airplane certification tests, and should be used to maximum advantage for early flight simulator validation. Certain critical tests that would normally be done early in the airplane certification program should be included to validate essential pilot training and certification maneuvers. These include cases where a pilot is expected to cope with an airplane failure mode or an engine failure. Flight test data that will be available early in the flight test program will depend on the airplane manufacturer's flight test program design and may not be the same in each case. The flight test program of the airplane manufacturer should include provisions for generation of very early flight test results for flight simulator validation.

f. The use of preliminary data is not indefinite. The airplane manufacturer's final data should be available within 12 months after the airplane's first entry into service or as agreed by the responsible Flight Standards office, the simulator sponsor, and the airplane manufacturer. When applying for interim qualification using preliminary data, the simulator sponsor and the responsible Flight Standards office should agree on the update program. This includes specifying that the final data update will be installed in the flight simulator within a period of 12 months following the final data release, unless special conditions exist and a different schedule is acceptable. The flight simulator performance and handling validation would then be based on data derived from flight tests or from other approved sources. Initial airplane systems data should be updated after engineering tests. Final airplane systems data should also be used for flight simulator programming and validation.

g. Flight simulator avionics should stay essentially in step with airplane avionics (hardware and software) updates. The permitted time lapse between airplane and flight simulator updates should be minimal. It may depend on the magnitude of the update and whether the QTG and pilot training and certification are affected. Differences in airplane and flight simulator avionics versions and the resulting effects on flight simulator qualification should be agreed between the simulator sponsor and the responsible Flight Standards office. Consultation with the flight simulator manufacturer is desirable throughout the qualification process.

h. The following describes an example of the design data and sources that might be used in the development of an interim qualification plan.

(1) The plan should consist of the development of a QTG based upon a mix of flight test and engineering simulation data. For data collected from specific airplane flight tests or other flights, the required design model or data changes necessary to support an acceptable Proof of Match (POM) should be generated by the airplane manufacturer.

(2) For proper validation of the two sets of data, the airplane manufacturer should compare their simulation model responses against the flight test data, when driven by the same control inputs and subjected to the same atmospheric conditions as recorded in the flight test. The model responses should result from a simulation where the following systems are run in an integrated fashion and are consistent with the design data released to the flight simulator manufacturer:

(a) Propulsion;

(b) Aerodynamics;

(c) Mass properties;

(d) Flight controls;

(e) Stability augmentation; and

(f) Brakes/landing gear.

i. A qualified test pilot should be used to assess handling qualities and performance evaluations for the qualification of flight simulators of new airplane types.

a. When a fully validated simulation (i.e., validated with flight test results) is modified due to changes to the simulated airplane configuration, the airplane manufacturer or other acceptable data supplier must coordinate with the responsible Flight Standards office if they propose to supply validation data from an "audited" engineering simulator/simulation to selectively supplement flight test data. The responsible Flight Standards office must be provided an opportunity to audit the engineering simulation or the engineering simulator used to generate the validation data. Validation data from an audited engineering simulation may be used for changes that are incremental in nature. Manufacturers or other data suppliers must be able to demonstrate that the predicted changes in aircraft performance are based on acceptable aeronautical principles with proven success history and valid outcomes. This must include comparisons of predicted and flight test validated data.

b. Airplane manufacturers or other acceptable data suppliers seeking to use an engineering simulator for simulation validation data as an alternative to flight-test derived validation data, must contact the responsible Flight Standards office and provide the following:

(1) A description of the proposed aircraft changes, a description of the proposed simulation model changes, and the use of an integral configuration management process, including a description of the actual simulation model modifications that includes a step-by-step description leading from the original model(s) to the current model(s).

(2) A schedule for review by the responsible Flight Standards office of the proposed plan and the subsequent validation data to establish acceptability of the proposal.

(3) Validation data from an audited engineering simulator/simulation to supplement specific segments of the flight test data.

c. To be qualified to supply engineering simulator validation data, for aerodynamic, engine, flight control, or ground handling models, an airplane manufacturer or other acceptable data supplier must:

(1) Be able to verify their ability able to:

(a) Develop and implement high fidelity simulation models; and

(b) Predict the handling and performance characteristics of an airplane with sufficient accuracy to avoid additional flight test activities for those handling and performance characteristics.

(2) Have an engineering simulator that:

(a) Is a physical entity, complete with a flight deck representative of the simulated class of airplane;

(b) Has controls sufficient for manual flight;

(c) Has models that run in an integrated manner;

(d) Has fully flight-test validated simulation models as the original or baseline simulation models;

(e) Has an out-of-the-flight deck visual system;

(f) Has actual avionics boxes interchangeable with the equivalent software simulations to support validation of released software;

(g) Uses the same models as released to the training community (which are also used to produce stand-alone proof-of-match and checkout documents);

(h) Is used to support airplane development and certification; and

(i) Has been found to be a high fidelity representation of the airplane by the manufacturer's pilots (or other acceptable data supplier), certificate holders, and the responsible Flight Standards office.

(3) Use the engineering simulator/simulation to produce a representative set of integrated proof-of-match cases.

(4) Use a configuration control system covering hardware and software for the operating components of the engineering simulator/simulation.

(5) Demonstrate that the predicted effects of the change(s) are within the provisions of sub-paragraph "a" of this section, and confirm that additional flight test data are not required.

d. Additional Requirements for Validation Data

(1) When used to provide validation data, an engineering simulator must meet the simulator standards currently applicable to training simulators except for the data package.

(2) The data package used must be:

(a) Comprised of the engineering predictions derived from the airplane design, development, or certification process;

(b) Based on acceptable aeronautical principles with proven success history and valid outcomes for aerodynamics, engine operations, avionics operations, flight control applications, or ground handling;

(c) Verified with existing flight-test data; and

(d) Applicable to the configuration of a production airplane, as opposed to a flight-test airplane.

(3) Where engineering simulator data are used as part of a QTG, an essential match must exist between the training simulator and the validation data.

(4) Training flight simulator(s) using these baseline and modified simulation models must be qualified to at least internationally recognized standards, such as contained in the ICAO Document 9625, the "Manual of Criteria for the Qualification of Flight Simulators."

a. Non-Flight-Test Tolerances

(1) If engineering simulator data or other non-flight-test data are used as an allowable form of reference validation data for the objective tests listed in Table A2A of this attachment, the data provider must supply a well-documented mathematical model and testing procedure that enables a replication of the engineering simulation results within 40% of the corresponding flight test tolerances.

b. Background

(1) The tolerances listed in Table A2A of this attachment are designed to measure the quality of the match using flight-test data as a reference.

(2) Good engineering judgment should be applied to all tolerances in any test. A test is failed when the results clearly fall outside of the prescribed tolerance(s).

(3) Engineering simulator data are acceptable because the same simulation models used to produce the reference data are also used to test the flight training simulator (i.e., the two sets of results should be "essentially" similar).

(4) The results from the two sources may differ for the following reasons:

(a) Hardware (avionics units and flight controls);

(b) Iteration rates;

(c) Execution order;

(d) Integration methods;

(e) Processor architecture;

(f) Digital drift, including:

(i) Interpolation methods;

(ii) Data handling differences; and

(iii) Auto-test trim tolerances.

(5) The tolerance limit between the reference data and the flight simulator results is generally 40 percent of the corresponding `flight-test' tolerances. However, there may be cases where the simulator models used are of higher fidelity, or the manner in which they are cascaded in the integrated testing loop have the effect of a higher fidelity, than those supplied by the data provider. Under these circumstances, it is possible that an error greater than 40 percent may be generated. An error greater than 40 percent may be acceptable if simulator sponsor can provide an adequate explanation.

(6) Guidelines are needed for the application of tolerances to engineering-simulator-generated validation data because:

(a) Flight-test data are often not available due to technical reasons;

(b) Alternative technical solutions are being advanced; and

(c) High costs.

a. Airplane manufacturers or other data suppliers should supply a validation data roadmap (VDR) document as part of the data package. A VDR document contains guidance material from the airplane validation data supplier recommending the best possible sources of data to be used as validation data in the QTG. A VDR is of special value when requesting interim qualification, qualification of simulators for airplanes certificated prior to 1992, and qualification of alternate engine or avionics fits. A sponsor seeking to have a device qualified in accordance with the standards contained in this QPS appendix should submit a VDR to the responsible Flight Standards office as early as possible in the planning stages. The responsible Flight Standards office is the final authority to approve the data to be used as validation material for the QTG.

b. The VDR should identify (in matrix format) sources of data for all required tests. It should also provide guidance regarding the validity of these data for a specific engine type, thrust rating configuration, and the revision levels of all avionics affecting airplane handling qualities and performance. The VDR should include rationale or explanation in cases where data or parameters are missing, engineering simulation data are to be used, flight test methods require explanation, or there is any deviation from data requirements. Additionally, the document should refer to other appropriate sources of validation data (e.g., sound and vibration data documents).

c. The Sample Validation Data Roadmap (VDR) for airplanes, shown in Table A2C, depicts a generic roadmap matrix identifying sources of validation data for an abbreviated list of tests. This document is merely a sample and does not provide actual data. A complete matrix should address all test conditions and provide actual data and data sources.

d. Two examples of rationale pages are presented in Appendix F of the IATA "Flight Simulator Design and Performance Data Requirements." These illustrate the type of airplane and avionics configuration information and descriptive engineering rationale used to describe data anomalies or provide an acceptable basis for using alternative data for QTG validation requirements.

(1) For a new airplane type, the majority of flight validation data are collected on the first airplane configuration with a "baseline" engine type. These data are then used to validate all flight simulators representing that airplane type.

(2) Additional flight test validation data may be needed for flight simulators representing an airplane with engines of a different type than the baseline, or for engines with thrust rating that is different from previously validated configurations.

(3) When a flight simulator with alternate engines is to be qualified, the QTG should contain tests against flight test validation data for selected cases where engine differences are expected to be significant.

(1) The following guidelines apply to flight simulators representing airplanes with alternate engine applications or with more than one engine type or thrust rating.

(2) Validation tests can be segmented into two groups, those that are dependent on engine type or thrust rating and those that are not.

(3) For tests that are independent of engine type or thrust rating, the QTG can be based on validation data from any engine application. Tests in this category should be designated as independent of engine type or thrust rating.

(4) For tests that are affected by engine type, the QTG should contain selected engine-specific flight test data sufficient to validate that particular airplane-engine configuration. These effects may be due to engine dynamic characteristics, thrust levels or engine-related airplane configuration changes. This category is primarily characterized by variations between different engine manufacturers' products, but also includes differences due to significant engine design changes from a previously flight-validated configuration within a single engine type. See Table A2D, Alternate Engine Validation Flight Tests in this section for a list of acceptable tests.

(5) Alternate engine validation data should be based on flight test data, except as noted in sub-paragraphs 13.c.(1) and (2), or where other data are specifically allowed (e.g., engineering simulator/simulation data). If certification of the flight characteristics of the airplane with a new thrust rating (regardless of percentage change) does require certification flight testing with a comprehensive stability and control flight instrumentation package, then the conditions described in Table A2D in this section should be obtained from flight testing and presented in the QTG. Flight test data, other than throttle calibration data, are not required if the new thrust rating is certified on the airplane without need for a comprehensive stability and control flight instrumentation package.

(6) As a supplement to the engine-specific flight tests listed in Table A2D and baseline engine-independent tests, additional engine-specific engineering validation data should be provided in the QTG, as appropriate, to facilitate running the entire QTG with the alternate engine configuration. The sponsor and the responsible Flight Standards office should agree in advance on the specific validation tests to be supported by engineering simulation data.

(7) A matrix or VDR should be provided with the QTG indicating the appropriate validation data source for each test.

(8) The flight test conditions in Table A2D are appropriate and should be sufficient to validate implementation of alternate engines in a flight simulator.

(1) The QTG must contain selected engine-specific flight test data sufficient to validate the alternative thrust level when:

(a) the engine type is the same, but the thrust rating exceeds that of a previously flight-test validated configuration by five percent (5%) or more; or

(b) the engine type is the same, but the thrust rating is less than the lowest previously flight-test validated rating by fifteen percent (15%) or more. See Table A2D for a list of acceptable tests.

(2) Flight test data is not required if the thrust increase is greater than 5%, but flight tests have confirmed that the thrust increase does not change the airplane's flight characteristics.

(3) Throttle calibration data (i.e., commanded power setting parameter versus throttle position) must be provided to validate all alternate engine types and engine thrust ratings that are higher or lower than a previously validated engine. Data from a test airplane or engineering test bench with the correct engine controller (both hardware and software) are required.

(1) For a new airplane type, the majority of flight validation data are collected on the first airplane configuration with a "baseline" flight-related avionics ship-set; (see subparagraph b.(2) of this section). These data are then used to validate all flight simulators representing that airplane type.

(2) Additional validation data may be required for flight simulators representing an airplane with avionics of a different hardware design than the baseline, or a different software revision than previously validated configurations.

(3) When a flight simulator with additional or alternate avionics configurations is to be qualified, the QTG should contain tests against validation data for selected cases where avionics differences are expected to be significant.

(1) The following guidelines apply to flight simulators representing airplanes with a revised avionics configuration, or more than one avionics configuration.

(2) The baseline validation data should be based on flight test data, except where other data are specifically allowed (e.g., engineering flight simulator data).

(3) The airplane avionics can be segmented into two groups, systems or components whose functional behavior contributes to the aircraft response presented in the QTG results, and systems that do not. The following avionics are examples of contributory systems for which hardware design changes or software revisions may lead to significant differences in the aircraft response relative to the baseline avionics configuration: Flight control computers and controllers for engines, autopilot, braking system, nosewheel steering system, and high lift system. Related avionics such as stall warning and augmentation systems should also be considered.

(4) The acceptability of validation data used in the QTG for an alternative avionics fit should be determined as follows:

(a) For changes to an avionics system or component that do not affect QTG validation test response, the QTG test can be based on validation data from the previously validated avionics configuration.

(b) For an avionics change to a contributory system, where a specific test is not affected by the change (e.g., the avionics change is a Built In Test Equipment (BITE) update or a modification in a different flight phase), the QTG test can be based on validation data from the previously-validated avionics configuration. The QTG should include authoritative justification (e.g., from the airplane manufacturer or system supplier) that this avionics change does not affect the test.

(c) For an avionics change to a contributory system, the QTG may be based on validation data from the previously-validated avionics configuration if no new functionality is added and the impact of the avionics change on the airplane response is small and based on acceptable aeronautical principles with proven success history and valid outcomes. This should be supplemented with avionics-specific validation data from the airplane manufacturer's engineering simulation, generated with the revised avionics configuration. The QTG should also include an explanation of the nature of the change and its effect on the airplane response.

(d) For an avionics change to a contributory system that significantly affects some tests in the QTG or where new functionality is added, the QTG should be based on validation data from the previously validated avionics configuration and supplemental avionics-specific flight test data sufficient to validate the alternate avionics revision. Additional flight test validation data may not be needed if the avionics changes were certified without the need for testing with a comprehensive flight instrumentation package. The airplane manufacturer should coordinate flight simulator data requirements, in advance with the responsible Flight Standards office.

(5) A matrix or "roadmap" should be provided with the QTG indicating the appropriate validation data source for each test. The roadmap should include identification of the revision state of those contributory avionics systems that could affect specific test responses if changed.

a. This paragraph explains how to determine the introduced transport delay through the flight simulator system so that it does not exceed a specific time delay. The transport delay should be measured from control inputs through the interface, through each of the host computer modules and back through the interface to motion, flight instrument, and visual systems. The transport delay should not exceed the maximum allowable interval.

b. Four specific examples of transport delay are:

(1) Simulation of classic non-computer controlled aircraft;

(2) Simulation of computer controlled aircraft using real airplane black boxes;

(3) Simulation of computer controlled aircraft using software emulation of airplane boxes;

(4) Simulation using software avionics or re-hosted instruments.

c. Figure A2C illustrates the total transport delay for a non-computer-controlled airplane or the classic transport delay test. Since there are no airplane-induced delays for this case, the total transport delay is equivalent to the introduced delay.

d. Figure A2D illustrates the transport delay testing method using the real airplane controller system.

e. To obtain the induced transport delay for the motion, instrument and visual signal, the delay induced by the airplane controller should be subtracted from the total transport delay. This difference represents the introduced delay and should not exceed the standards prescribed in Table A1A.

f. Introduced transport delay is measured from the flight deck control input to the reaction of the instruments and motion and visual systems (See Figure A2C).

g. The control input may also be introduced after the airplane controller system and the introduced transport delay measured directly from the control input to the reaction of the instruments, and simulator motion and visual systems (See Figure A2D).

h. Figure A2E illustrates the transport delay testing method used on a flight simulator that uses a software emulated airplane controller system.

i. It is not possible to measure the introduced transport delay using the simulated airplane controller system architecture for the pitch, roll and yaw axes. Therefore, the signal should be measured directly from the pilot controller. The flight simulator manufacturer should measure the total transport delay and subtract the inherent delay of the actual airplane components because the real airplane controller system has an inherent delay provided by the airplane manufacturer. The flight simulator manufacturer should ensure that the introduced delay does not exceed the standards prescribed in Table A1A.

j. Special measurements for instrument signals for flight simulators using a real airplane instrument display system instead of a simulated or re-hosted display. For flight instrument systems, the total transport delay should be measured and the inherent delay of the actual airplane components subtracted to ensure that the introduced delay does not exceed the standards prescribed in Table A1A.

(1) Figure A2FA illustrates the transport delay procedure without airplane display simulation. The introduced delay consists of the delay between the control movement and the instrument change on the data bus.

(2) Figure A2FB illustrates the modified testing method required to measure introduced delay due to software avionics or re-hosted instruments. The total simulated instrument transport delay is measured and the airplane delay should be subtracted from this total. This difference represents the introduced delay and should not exceed the standards prescribed in Table A1A. The inherent delay of the airplane between the data bus and the displays is indicated in figure A2FA. The display manufacturer should provide this delay time.

k. Recorded signals. The signals recorded to conduct the transport delay calculations should be explained on a schematic block diagram. The flight simulator manufacturer should also provide an explanation of why each signal was selected and how they relate to the above descriptions.

l. Interpretation of results. Flight simulator results vary over time from test to test due to "sampling uncertainty." All flight simulators run at a specific rate where all modules are executed sequentially in the host computer. The flight controls input can occur at any time in the iteration, but these data will not be processed before the start of the new iteration. For example, a flight simulator running at 60 Hz may have a difference of as much as 16.67 msec between test results. This does not mean that the test has failed. Instead, the difference is attributed to variations in input processing. In some conditions, the host simulator and the visual system do not run at the same iteration rate, so the output of the host computer to the visual system will not always be synchronized.

m. The transport delay test should account for both daylight and night modes of operation of the visual system. In both cases, the tolerances prescribed in Table A1A must be met and the motion response should occur before the end of the first video scan containing new information.

(1) The MQTG is created during the initial evaluation of a flight simulator. This is the master document, as amended, to which flight simulator continuing qualification evaluation test results are compared.

(2) The currently accepted method of presenting continuing qualification evaluation test results is to provide flight simulator results over-plotted with reference data. Test results are carefully reviewed to determine if the test is within the specified tolerances. This can be a time consuming process, particularly when reference data exhibits rapid variations or an apparent anomaly requiring engineering judgment in the application of the tolerances. In these cases, the solution is to compare the results to the MQTG. The continuing qualification results are compared to the results in the MQTG for acceptance. The flight simulator operator and the responsible Flight Standards office should look for any change in the flight simulator performance since initial qualification.

(1) Flight simulator operators are encouraged to over-plot continuing qualification validation test results with MQTG flight simulator results recorded during the initial evaluation and as amended. Any change in a validation test will be readily apparent. In addition to plotting continuing qualification validation test and MQTG results, operators may elect to plot reference data as well.

(2) There are no suggested tolerances between flight simulator continuing qualification and MQTG validation test results. Investigation of any discrepancy between the MQTG and continuing qualification flight simulator performance is left to the discretion of the flight simulator operator and the responsible Flight Standards office.

(3) Differences between the two sets of results, other than variations attributable to repeatability issues that cannot be explained, should be investigated.

(4) The flight simulator should retain the ability to over-plot both automatic and manual validation test results with reference data.

a. Sponsors are not required to use the alternative data sources, procedures, and instrumentation. However, a sponsor may choose to use one or more of the alternative sources, procedures, and instrumentation described in Table A2E.

b. It has become standard practice for experienced simulator manufacturers to use modeling techniques to establish data bases for new simulator configurations while awaiting the availability of actual flight test data. The data generated from the aerodynamic modeling techniques is then compared to the flight test data when it becomes available. The results of such comparisons have become increasingly consistent, indicating that these techniques, applied with the appropriate experience, are dependable and accurate for the development of aerodynamic models for use in Level A and Level B simulators.

c. Based on this history of successful comparisons, the responsible Flight Standards office has concluded that those who are experienced in the development of aerodynamic models may use modeling techniques to alter the method for acquiring flight test data for Level A or Level B simulators.

d. The information in Table A2E (Alternative Data Sources, Procedures, and Instrumentation) is presented to describe an acceptable alternative to data sources for simulator modeling and validation and an acceptable alternative to the procedures and instrumentation traditionally used to gather such modeling and validation data.

(1) Alternative data sources that may be used for part or all of a data requirement are the Airplane Maintenance Manual, the Airplane Flight Manual (AFM), Airplane Design Data, the Type Inspection Report (TIR), Certification Data or acceptable supplemental flight test data.

(2) The sponsor should coordinate with the responsible Flight Standards office prior to using alternative data sources in a flight test or data gathering effort.

e. The responsible Flight Standards office position regarding the use of these alternative data sources, procedures, and instrumentation is based on the following presumptions:

(1) Data gathered through the alternative means does not require angle of attack (AOA) measurements or control surface position measurements for any flight test. However, AOA can be sufficiently derived if the flight test program ensures the collection of acceptable level, unaccelerated, trimmed flight data. All of the simulator time history tests that begin in level, unaccelerated, and trimmed flight, including the three basic trim tests and "fly-by" trims, can be a successful validation of angle of attack by comparison with flight test pitch angle. (Note: Due to the criticality of angle of attack in the development of the ground effects model, particularly critical for normal landings and landings involving cross-control input applicable to Level B simulators, stable "fly-by" trim data will be the acceptable norm for normal and cross-control input landing objective data for these applications.)

(2) The use of a rigorously defined and fully mature simulation controls system model that includes accurate gearing and cable stretch characteristics (where applicable), determined from actual aircraft measurements. Such a model does not require control surface position measurements in the flight test objective data in these limited applications.

f. The sponsor is urged to contact the responsible Flight Standards office for clarification of any issue regarding airplanes with reversible control systems. Table A2E is not applicable to Computer Controlled Aircraft FFSs.

g. Utilization of these alternate data sources, procedures, and instrumentation (Table A2E) does not relieve the sponsor from compliance with the balance of the information contained in this document relative to Level A or Level B FFSs.

h. The term "inertial measurement system" is used in the following table to include the use of a functional global positioning system (GPS).

i. Synchronized video for the use of alternative data sources, procedures, and instrumentation should have:

(1) Sufficient resolution to allow magnification of the display to make appropriate measurement and comparisons; and

(2) Sufficient size and incremental marking to allow similar measurement and comparison. The detail provided by the video should provide sufficient clarity and accuracy to measure the necessary parameter(s) to at leastof the tolerance authorized for the specific test being conducted and allow an integration of the parameter(s) in question to obtain a rate of change.

a. Except for special use airport models, described as Class III, all airport models required by this part must be representations of real-world, operational airports or representations of fictional airports and must meet the requirements set out in Tables A3B or A3C of this attachment, as appropriate.

b. If fictional airports are used, the sponsor must ensure that navigational aids and all appropriate maps, charts, and other navigational reference material for the fictional airports (and surrounding areas as necessary) are compatible, complete, and accurate with respect to the visual presentation of the airport model of this fictional airport. An SOC must be submitted that addresses navigation aid installation and performance and other criteria (including obstruction clearance protection) for all instrument approaches to the fictional airports that are available in the simulator. The SOC must reference and account for information in the terminal instrument procedures manual and the construction and availability of the required maps, charts, and other navigational material. This material must be clearly marked "for training purposes only."

c. When the simulator is being used by an instructor or evaluator for purposes of training, checking, or testing under this chapter, only airport models classified as Class I, Class II, or Class III may be used by the instructor or evaluator. Detailed descriptions/definitions of these classifications are found in Appendix F of this part.

d. When a person sponsors an FFS maintained by a person other than a U.S. certificate holder, the sponsor is accountable for that FFS originally meeting, and continuing to meet, the criteria under which it was originally qualified and the appropriate Part 60 criteria, including the airport models that may be used by instructors or evaluators for purposes of training, checking, or testing under this chapter.

e. Neither Class II nor Class III airport visual models are required to appear on the SOQ, and the method used for keeping instructors and evaluators apprised of the airport models that meet Class II or Class III requirements on any given simulator is at the option of the sponsor, but the method used must be available for review by the TPAA.

f. When an airport model represents a real world airport and a permanent change is made to that real world airport (e.g., a new runway, an extended taxiway, a new lighting system, a runway closure) without a written extension grant from the responsible Flight Standards office (described in paragraph 1.g. of this section), an update to that airport model must be made in accordance with the following time limits:

(1) For a new airport runway, a runway extension, a new airport taxiway, a taxiway extension, or a runway/taxiway closure-within 90 days of the opening for use of the new airport runway, runway extension, new airport taxiway, or taxiway extension; or within 90 days of the closure of the runway or taxiway.

(2) For a new or modified approach light system-within 45 days of the activation of the new or modified approach light system.

(3) For other facility or structural changes on the airport (e.g., new terminal, relocation of Air Traffic Control Tower)-within 180 days of the opening of the new or changed facility or structure.

g. If a sponsor desires an extension to the time limit for an update to a visual scene or airport model or has an objection to what must be updated in the specific airport model requirement, the sponsor must provide a written extension request to the responsible Flight Standards office stating the reason for the update delay and a proposed completion date, or explain why the update is not necessary (i.e., why the identified airport change will not have an impact on flight training, testing, or checking). A copy of this request or objection must also be sent to the POI/TCPM. The responsible Flight Standards office will send the official response to the sponsor and a copy to the POI/TCPM. If there is an objection, after consultation with the appropriate POI/TCPM regarding the training, testing, or checking impact, the responsible Flight Standards office will send the official response to the sponsor and a copy to the POI/TCPM.

a. The subjective tests provide a basis for evaluating the capability of the simulator to perform over a typical utilization period; determining that the simulator accurately simulates each required maneuver, procedure, or task; and verifying correct operation of the simulator controls, instruments, and systems. The items listed in the following Tables are for simulator evaluation purposes only. They may not be used to limit or exceed the authorizations for use of a given level of simulator, as described on the SOQ, or as approved by the TPAA.

b. The tests in Table A3A, Operations Tasks, in this attachment, address pilot functions, including maneuvers and procedures (called flight tasks), and are divided by flight phases. The performance of these tasks by the responsible Flight Standards office includes an operational examination of the visual system and special effects. There are flight tasks included to address some features of advanced technology airplanes and innovative training programs. For example, "high angle-of-attack maneuvering" is included to provide a required alternative to "approach to stalls" for airplanes employing flight envelope protection functions.

c. The tests in Table A3A, Operations Tasks, and Table A3G, Instructor Operating Station of this attachment, address the overall function and control of the simulator including the various simulated environmental conditions; simulated airplane system operations (normal, abnormal, and emergency); visual system displays; and special effects necessary to meet flight crew training, evaluation, or flight experience requirements.

d. All simulated airplane systems functions will be assessed for normal and, where appropriate, alternate operations. Normal, abnormal, and emergency operations associated with a flight phase will be assessed during the evaluation of flight tasks or events within that flight phase. Simulated airplane systems are listed separately under "Any Flight Phase" to ensure appropriate attention to systems checks. Operational navigation systems (including inertial navigation systems, global positioning systems, or other long-range systems) and the associated electronic display systems will be evaluated if installed. The pilot will include in his report to the TPAA, the effect of the system operation and any system limitation.

e. Simulators demonstrating a satisfactory circling approach will be qualified for the circling approach maneuver and may be approved for such use by the TPAA in the sponsor's FAA-approved flight training program. To be considered satisfactory, the circling approach will be flown at maximum gross weight for landing, with minimum visibility for the airplane approach category, and must allow proper alignment with a landing runway at least 90&#xB0; different from the instrument approach course while allowing the pilot to keep an identifiable portion of the airport in sight throughout the maneuver (reference-14 CFR 91.175(e)).

f. At the request of the TPAA, the responsible Flight Standards office may assess a device to determine if it is capable of simulating certain training activities in a sponsor's training program, such as a portion of a Line Oriented Flight Training (LOFT) scenario. Unless directly related to a requirement for the qualification level, the results of such an evaluation would not affect the qualification level of the simulator. However, if the responsible Flight Standards office determines that the simulator does not accurately simulate that training activity, the simulator would not be approved for that training activity.

g. The FAA intends to allow the use of Class III airport models when the sponsor provides the TPAA (or other regulatory authority) an appropriate analysis of the skills, knowledge, and abilities (SKAs) necessary for competent performance of the tasks in which this particular media element is used. The analysis should describe the ability of the FFS/visual media to provide an adequate environment in which the required SKAs are satisfactorily performed and learned. The analysis should also include the specific media element, such as the airport model.

h. The TPAA may accept Class III airport models without individual observation provided the sponsor provides the TPAA with an acceptable description of the process for determining the acceptability of a specific airport model, outlines the conditions under which such an airport model may be used, and adequately describes what restrictions will be applied to each resulting airport or landing area model. Examples of situations that may warrant Class_III model designation by the TPAA include the following:

(a) Training, testing, or checking on very low visibility operations, including SMGCS operations.

(b) Instrument operations training (including instrument takeoff, departure, arrival, approach, and missed approach training, testing, or checking) using-

(i) A specific model that has been geographically "moved" to a different location and aligned with an instrument procedure for another airport.

(ii) A model that does not match changes made at the real-world airport (or landing area for helicopters) being modeled.

(iii) A model generated with an "off-board" or an "on-board" model development tool (by providing proper latitude/longitude reference; correct runway or landing area orientation, length, width, marking, and lighting information; and appropriate adjacent taxiway location) to generate a facsimile of a real world airport or landing area.

i. Previously qualified simulators with certain early generation Computer Generated Image (CGI) visual systems, are limited by the capability of the Image Generator or the display system used. These systems are:

(1) Early CGI visual systems that are excepted from the requirement of including runway numbers as a part of the specific runway marking requirements are:

(a) Link NVS and DNVS.

(b) Novoview 2500 and 6000.

(c) FlightSafety VITAL series up to, and including, VITAL III, but not beyond.

(d) Redifusion SP1, SP1T, and SP2.

(2) Early CGI visual systems are excepted from the requirement of including runway numbers unless the runways are used for LOFT training sessions. These LOFT airport models require runway numbers but only for the specific runway end (one direction) used in the LOFT session. The systems required to display runway numbers only for LOFT scenes are:

(a) FlightSafety VITAL IV.

(b) Redifusion SP3 and SP3T.

(c) Link-Miles Image II.

(3) The following list of previously qualified CGI and display systems are incapable of generating blue lights. These systems are not required to have accurate taxi-way edge lighting:

(a) Redifusion SP1.

(b) FlightSafety Vital IV.

(c) Link-Miles Image II and Image IIT

(d) XKD displays (even though the XKD image generator is capable of generating blue colored lights, the display cannot accommodate that color).

a. The following is an example test schedule for an Initial/Upgrade evaluation that covers the majority of the requirements set out in the Functions and Subjective test requirements. It is not intended that the schedule be followed line by line, rather, the example should be used as a guide for preparing a schedule that is tailored to the airplane, sponsor, and training task.

b. Functions and subjective tests should be planned. This information has been organized as a reference document with the considerations, methods, and evaluation notes for each individual aspect of the simulator task presented as an individual item. In this way the evaluator can design his or her own test plan, using the appropriate sections to provide guidance on method and evaluation criteria. Two aspects should be present in any test plan structure:

(1) An evaluation of the simulator to determine that it replicates the aircraft and performs reliably for an uninterrupted period equivalent to the length of a typical training session.

(2) The simulator should be capable of operating reliably after the use of training device functions such as repositions or malfunctions.

c. A detailed understanding of the training task will naturally lead to a list of objectives that the simulator should meet. This list will form the basis of the test plan. Additionally, once the test plan has been formulated, the initial conditions and the evaluation criteria should be established. The evaluator should consider all factors that may have an influence on the characteristics observed during particular training tasks in order to make the test plan successful.

(1) Airport.

(2) QNH.

(3) Temperature.

(4) Wind/Crosswind.

(5) Zero Fuel Weight /Fuel/Gross Weight /Center of Gravity.

(1) Documentation of Simulator.

(a) Simulator Acceptance Test Manuals.

(b) Simulator Approval Test Guide.

(c) Technical Logbook Open Item List.

(d) Daily Functional Pre-flight Check.

(2) Documentation of User/Carrier Flight Logs.

(a) Simulator Operating/Instructor Manual.

(b) Difference List (Aircraft/Simulator).

(c) Flight Crew Operating Manuals.

(d) Performance Data for Different Fields.

(e) Crew Training Manual.

(f) Normal/Abnormal/Emergency Checklists.

(3) Simulator External Checks.

(a) Appearance and Cleanliness.

(b) Stairway/Access Bridge.

(c) Emergency Rope Ladders.

(d) "Motion On"/"Flight in Progress" Lights.

(4) Simulator Internal Checks.

(a) Cleaning/Disinfecting Towels (for cleaning oxygen masks).

(b) Flight deck Layout (compare with difference list).

(5) Equipment.

(a) Quick Donning Oxygen Masks.

(b) Head Sets.

(c) Smoke Goggles.

(d) Sun Visors.

(e) Escape Rope.

(f) Chart Holders.

(g) Flashlights.

(h) Fire Extinguisher (inspection date).

(i) Crash Axe.

(j) Gear Pins.

(1) Batteries and Static Inverter.

(2) APU Start with Battery.

(3) APU Shutdown using Fire Handle.

(4) External Power Connection.

(5) APU Start with External Power.

(6) Abnormal APU Start/Operation.

(1) Flight deck Preparation Checks.

(2) FMC Programming.

(3) Communications and Navigational Aids Checks.

(1) Before Start Checks.

(2) Battery start with Ground Air Supply Unit.

(3) Engine Crossbleed Start.

(4) Normal Engine Start.

(5) Abnormal Engine Starts.

(6) Engine Idle Readings.

(7) After Start Checks.

(1) Pushback/Powerback.

(2) Taxi Checks.

(3) Ground Handling Check:

(a) Power required to initiate ground roll.

(b) Thrust response.

(c) Nosewheel and Pedal Steering.

(d) Nosewheel Scuffing.

(e) Perform 180 degree turns.

(f) Brakes Response and Differential Braking using Normal, Alternate and Emergency.

(g) Brake Systems.

(h) Eye height and fore/aft position.

(4) Runway Roughness.
